import os
from typing import List, Dict, Any, Optional, Tuple
import re
import json
from fastapi import FastAPI, Request
from pydantic import BaseModel
import logging

# LangChain ê´€ë ¨ ì„í¬íŠ¸ (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)
try:
    from langchain_ollama import ChatOllama
    from langchain.agents import create_tool_calling_agent, AgentExecutor
    from langchain.tools import Tool
    from langchain_core.prompts import ChatPromptTemplate
    from langchain.schema import Document
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"LangChain ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜: {e}")
    LANGCHAIN_AVAILABLE = False

# ë²¡í„°ìŠ¤í† ì–´ ê´€ë ¨ ì„í¬íŠ¸ (ì„ íƒì )
try:
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    VECTORSTORE_AVAILABLE = True
except ImportError:
    try:
        from langchain_community.vectorstores import FAISS
        from langchain.embeddings import HuggingFaceEmbeddings
        VECTORSTORE_AVAILABLE = True
    except ImportError:
        print("Warning: ë²¡í„°ìŠ¤í† ì–´ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
        VECTORSTORE_AVAILABLE = False

# KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸° ì¶”ê°€
try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
except ImportError:
    print("Warning: KoNLPy not installed. ê·œì¹™ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    KONLPY_AVAILABLE = False

# KIPRIS API ëª¨ë“ˆ ì„í¬íŠ¸
from kipris_api import search_and_extract_patents

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

class QueryRequest(BaseModel):
    input: str
    chat_history: List[Tuple[str, str]] = []

class KoreanKeywordExtractor:
    """í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.okt = None
        self.konlpy_available = KONLPY_AVAILABLE  # ê¸€ë¡œë²Œ ë³€ìˆ˜ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥
        
        if self.konlpy_available:
            try:
                # Oktê°€ ê°€ì¥ ì•ˆì •ì ì´ê³  ë¹ ë¦„
                self.okt = Okt()
                logger.info("KoNLPy Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"KoNLPy ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.konlpy_available = False
        
        # ê¸°ìˆ  ë¶„ì•¼ë³„ í•µì‹¬ í‚¤ì›Œë“œ ì‚¬ì „
        self.tech_keywords = {
            'belt': ['ë²¨íŠ¸', 'íƒ€ì´ë°ë²¨íŠ¸', 'Vë²¨íŠ¸', 'ì»¨ë² ì´ì–´ë²¨íŠ¸', 'ì „ë™ë²¨íŠ¸', 'ì²´ì¸', 'ë™ë ¥ì „ë‹¬'],
            'conveyor': ['ì»¨ë² ì´ì–´', 'ì´ì†¡', 'ìš´ë°˜', 'ë°˜ì†¡', 'ì»¨ë² ì´ì–´ì‹œìŠ¤í…œ'],
            'automation': ['ìë™í™”', 'ë¡œë´‡', 'ê·¸ë¦¬í¼', 'í”½ì•¤í”Œë ˆì´ìŠ¤', 'ë§¤ë‹ˆí“°ë ˆì´í„°'],
            'tension': ['ì¥ë ¥', 'í…ì…˜', 'ì¡°ì ˆ', 'ì œì–´', 'ëª¨ë‹ˆí„°ë§'],
            'material': ['ê³ ë¬´', 'ì‹¤ë¦¬ì½˜', 'ì¹´ë³¸', 'ì¼€ë¸”ë¼', 'ë³µí•©ì†Œì¬', 'ë‚´ì—´'],
            'system': ['ì‹œìŠ¤í…œ', 'ì¥ì¹˜', 'ê¸°êµ¬', 'ë©”ì»¤ë‹ˆì¦˜', 'êµ¬ë™']
        }
        
        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
        self.stopwords = {
            'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ì´', 'ê·¸', 'ì €', 'ì˜', 'ë¥¼', 'ì„', 'ê°€', 'ì´', 'ì—', 'ì—ì„œ', 
            'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'í•œí…Œ', 'ì—ê²Œ', 'ê»˜',
            'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤', 'ê°™ë‹¤', 'ë‹¤ë¥´ë‹¤',
            'ì¢€', 'ë”', 'ê°€ì¥', 'ë§¤ìš°', 'ì •ë§', 'ì§„ì§œ', 'ì™„ì „', 'ì•„ì£¼', 'ë„ˆë¬´',
            'ì°¾ë‹¤', 'ê²€ìƒ‰', 'ì•Œë ¤ì£¼ë‹¤', 'ë³´ì—¬ì£¼ë‹¤', 'ì£¼ë‹¤', 'í•´ì£¼ë‹¤', 'ë“œë¦¬ë‹¤'
        }
    
    def extract_keywords_with_morphology(self, text: str) -> List[str]:
        """í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not self.okt:
            return self.extract_keywords_with_rules(text)
        
        try:
            # ëª…ì‚¬ì™€ ë³µí•©ëª…ì‚¬ ì¶”ì¶œ
            nouns = self.okt.nouns(text)
            
            # 2ê¸€ì ì´ìƒì˜ ëª…ì‚¬ë§Œ í•„í„°ë§
            filtered_nouns = [noun for noun in nouns 
                            if len(noun) >= 2 and noun not in self.stopwords]
            
            # ê¸°ìˆ  í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
            priority_keywords = []
            general_keywords = []
            
            for noun in filtered_nouns:
                is_tech_keyword = False
                for category, keywords in self.tech_keywords.items():
                    if any(keyword in noun or noun in keyword for keyword in keywords):
                        priority_keywords.append(noun)
                        is_tech_keyword = True
                        break
                
                if not is_tech_keyword:
                    general_keywords.append(noun)
            
            # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œë¥¼ ì•ì— ë°°ì¹˜
            result = priority_keywords + general_keywords
            
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
            seen = set()
            unique_keywords = []
            for keyword in result:
                if keyword not in seen:
                    seen.add(keyword)
                    unique_keywords.append(keyword)
            
            return unique_keywords[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
            
        except Exception as e:
            logger.warning(f"í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ fallback: {e}")
            return self.extract_keywords_with_rules(text)
    
    def extract_keywords_with_rules(self, text: str) -> List[str]:
        """ê·œì¹™ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ê¸° ì—†ì„ ë•Œ fallback)"""
        
        # 1. ì •ê·œì‹ì„ í†µí•œ ê¸°ë³¸ í‚¤ì›Œë“œ íŒ¨í„´ ì¶”ì¶œ
        patterns = [
            r'([ê°€-í£]{2,}ë²¨íŠ¸)',  # ~ë²¨íŠ¸
            r'([ê°€-í£]{2,}ì»¨ë² ì´ì–´)', # ~ì»¨ë² ì´ì–´  
            r'([ê°€-í£]{2,}ë¡œë´‡)',    # ~ë¡œë´‡
            r'([ê°€-í£]{2,}ì‹œìŠ¤í…œ)',  # ~ì‹œìŠ¤í…œ
            r'([ê°€-í£]{2,}ì¥ì¹˜)',    # ~ì¥ì¹˜
            r'([ê°€-í£]{2,}ê¸°êµ¬)',    # ~ê¸°êµ¬
        ]
        
        keywords = []
        
        # íŒ¨í„´ ë§¤ì¹­
        for pattern in patterns:
            matches = re.findall(pattern, text)
            keywords.extend(matches)
        
        # 2. ê¸°ìˆ  í‚¤ì›Œë“œ ì‚¬ì „ê³¼ ë§¤ì¹­
        for category, tech_words in self.tech_keywords.items():
            for tech_word in tech_words:
                if tech_word in text:
                    keywords.append(tech_word)
        
        # 3. í•œê¸€ ëª…ì‚¬í˜• ë‹¨ì–´ ì¶”ì¶œ (2-6ê¸€ì)
        korean_words = re.findall(r'[ê°€-í£]{2,6}', text)
        filtered_words = [word for word in korean_words 
                         if word not in self.stopwords and len(word) >= 2]
        
        keywords.extend(filtered_words)
        
        # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
        seen = set()
        unique_keywords = []
        for keyword in keywords:
            if keyword not in seen:
                seen.add(keyword)
                unique_keywords.append(keyword)
        
        return unique_keywords[:5]

# ì „ì—­ í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤
keyword_extractor = KoreanKeywordExtractor()

def extract_search_keywords(user_input: str) -> str:
    """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ê²€ìƒ‰ìš© í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ë„êµ¬"""
    
    logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘: {user_input}")
    
    # 1. í˜•íƒœì†Œ ë¶„ì„ ë˜ëŠ” ê·œì¹™ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
    if keyword_extractor.konlpy_available:
        keywords = keyword_extractor.extract_keywords_with_morphology(user_input)
        method = "í˜•íƒœì†Œë¶„ì„"
    else:
        keywords = keyword_extractor.extract_keywords_with_rules(user_input)
        method = "ê·œì¹™ê¸°ë°˜"
    
    # 2. ìˆ«ì ì •ë³´ ì¶”ì¶œ (ëª‡ ê±´, ëª‡ ê°œ ë“±)
    number_match = re.search(r'(\d+)\s*(?:ê±´|ê°œ|ê°œ|ê°€ì§€|ì¢…ë¥˜)', user_input)
    num_results = int(number_match.group(1)) if number_match else 5
    
    # í‚¤ì›Œë“œë¥¼ ê³µë°±ìœ¼ë¡œ ì—°ê²°
    search_query = ' '.join(keywords) if keywords else user_input
    
    logger.info(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ ({method}): {keywords}")
    logger.info(f"ìƒì„±ëœ ê²€ìƒ‰ì¿¼ë¦¬: {search_query}")
    
    result = {
        "search_query": search_query,
        "keywords": keywords,
        "num_results": num_results,
        "extraction_method": method
    }
    
    return json.dumps(result, ensure_ascii=False)

def search_kipris_patents(query_info: str) -> str:
    """ê°œì„ ëœ KIPRIS íŠ¹í—ˆ ê²€ìƒ‰ ë„êµ¬ - REST_API.py ê¸°ë°˜ í‚¤ ë§¤í•‘"""
    
    try:
        # query_infoëŠ” JSON ë¬¸ìì—´ì´ë¯€ë¡œ íŒŒì‹±
        query_data = json.loads(query_info)
        search_query = query_data.get("search_query", "")
        num_results = query_data.get("num_results", 5)
        keywords = query_data.get("keywords", [])
        method = query_data.get("extraction_method", "")
        
        if not search_query.strip():
            return "ê²€ìƒ‰í•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        logger.info(f"KIPRIS ê²€ìƒ‰ ì‹¤í–‰: '{search_query}' ({num_results}ê±´)")
        
        # KIPRIS API í˜¸ì¶œ
        patents = search_and_extract_patents(search_query, num_results)
        
        if not patents:
            # í‚¤ì›Œë“œê°€ ìˆë‹¤ë©´ ê°œë³„ì ìœ¼ë¡œ ì¬ì‹œë„
            if keywords:
                for keyword in keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„
                    logger.info(f"ëŒ€ì²´ ê²€ìƒ‰ ì‹œë„: {keyword}")
                    patents = search_and_extract_patents(keyword, num_results)
                    if patents:
                        search_query = keyword  # ì„±ê³µí•œ í‚¤ì›Œë“œë¡œ ì—…ë°ì´íŠ¸
                        break
        
        if not patents:
            return f"'{search_query}' ê´€ë ¨ íŠ¹í—ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”."
        
        # ê²°ê³¼ í¬ë§·íŒ… (REST_API.py ìŠ¤íƒ€ì¼)
        result = f"ğŸ” **ê²€ìƒ‰ì–´**: {search_query} ({method})\n"
        result += f"ğŸ“„ **ê²€ìƒ‰ ê²°ê³¼**: {len(patents)}ê±´\n\n"
        
        for i, patent in enumerate(patents, 1):
            if isinstance(patent, dict) and 'error' not in patent:
                # REST_API.pyì™€ ë™ì¼í•œ í‚¤ ì‚¬ìš©
                title = patent.get('ë°œëª…ëª…ì¹­', 'N/A')
                applicant = patent.get('ì¶œì›ì¸', 'N/A')
                app_number = patent.get('ì¶œì›ë²ˆí˜¸', 'N/A')
                app_date = patent.get('ì¶œì›ì¼ì', 'N/A')
                pub_number = patent.get('ê³µê°œë²ˆí˜¸', 'N/A')
                pub_date = patent.get('ê³µê°œì¼ì', 'N/A')
                reg_number = patent.get('ë“±ë¡ë²ˆí˜¸', 'N/A')
                reg_date = patent.get('ë“±ë¡ì¼ì', 'N/A')
                reg_status = patent.get('ë“±ë¡ìƒíƒœ', 'N/A')
                abstract = patent.get('ì´ˆë¡', 'N/A')
                
                result += f"**[{i}] {title}**\n"
                result += f"   â€¢ ì¶œì›ì¸: {applicant}\n"
                result += f"   â€¢ ì¶œì›ë²ˆí˜¸: {app_number} (ì¶œì›ì¼: {app_date})\n"
                
                if pub_number != 'N/A':
                    result += f"   â€¢ ê³µê°œë²ˆí˜¸: {pub_number} (ê³µê°œì¼: {pub_date})\n"
                
                if reg_number != 'N/A':
                    result += f"   â€¢ ë“±ë¡ë²ˆí˜¸: {reg_number} (ë“±ë¡ì¼: {reg_date})\n"
                
                result += f"   â€¢ ë“±ë¡ìƒíƒœ: {reg_status}\n"
                
                if abstract != 'N/A' and len(abstract) > 150:
                    abstract = abstract[:150] + "..."
                result += f"   â€¢ ì´ˆë¡: {abstract}\n\n"
            else:
                # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš°
                logger.error(f"íŠ¹í—ˆ ë°ì´í„° ì˜¤ë¥˜: {patent}")
                result += f"**[{i}] ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜**\n\n"
        
        return result
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return "í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    except Exception as e:
        logger.error(f"KIPRIS ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return f"íŠ¹í—ˆ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
def initialize_rag_system():
    """ë‚´ë¶€ ë¬¸ì„œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    
    if not VECTORSTORE_AVAILABLE:
        logger.warning("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # knowledge.txt íŒŒì¼ ì½ê¸°
        if os.path.exists("knowledge.txt"):
            with open("knowledge.txt", "r", encoding="utf-8") as f:
                content = f.read()
            
            # ë¬¸ì„œ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
            )
            
            docs = [Document(page_content=content)]
            split_docs = text_splitter.split_documents(docs)
            
            # ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
            
            vectorstore = FAISS.from_documents(split_docs, embeddings)
            
            logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            return vectorstore
        else:
            logger.warning("knowledge.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
    except Exception as e:
        logger.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

# RAG ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
vectorstore = initialize_rag_system()

def search_internal_knowledge(query: str) -> str:
    """ë‚´ë¶€ ì§€ì‹ ë¬¸ì„œ ê²€ìƒ‰"""
    
    if not vectorstore:
        # RAG ì‹œìŠ¤í…œì´ ì—†ì„ ë•ŒëŠ” ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´
        try:
            if os.path.exists("knowledge.txt"):
                with open("knowledge.txt", "r", encoding="utf-8") as f:
                    content = f.read()
                
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
                query_lower = query.lower()
                lines = content.split('\n')
                relevant_lines = []
                
                for line in lines:
                    if any(keyword in line.lower() for keyword in query_lower.split()):
                        relevant_lines.append(line.strip())
                
                if relevant_lines:
                    result = "ğŸ“š **ë‚´ë¶€ ì§€ì‹ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ (í‚¤ì›Œë“œ ë§¤ì¹­):**\n\n"
                    for i, line in enumerate(relevant_lines[:5], 1):  # ìµœëŒ€ 5ê°œ
                        if line:
                            result += f"**[{i}]** {line}\n\n"
                    return result
                else:
                    return "ê´€ë ¨ ë‚´ë¶€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                return "ë‚´ë¶€ ì§€ì‹ ë¬¸ì„œ(knowledge.txt)ê°€ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"ë‚´ë¶€ ì§€ì‹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return f"ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    try:
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        docs = vectorstore.similarity_search(query, k=3)
        
        if not docs:
            return "ê´€ë ¨ ë‚´ë¶€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        result = "ğŸ“š **ë‚´ë¶€ ì§€ì‹ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:**\n\n"
        for i, doc in enumerate(docs, 1):
            result += f"**[{i}]** {doc.page_content.strip()}\n\n"
        
        return result
        
    except Exception as e:
        logger.error(f"ë‚´ë¶€ ì§€ì‹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return f"ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ë„êµ¬ ì •ì˜
tools = [
    Tool(
        name="keyword_extractor",
        description="ì‚¬ìš©ìì˜ í•œêµ­ì–´ ì§ˆë¬¸ì—ì„œ íŠ¹í—ˆ ê²€ìƒ‰ì— ì í•©í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. íŠ¹í—ˆ ê²€ìƒ‰ ì „ì— ë°˜ë“œì‹œ ì´ ë„êµ¬ë¥¼ ë¨¼ì € ì‚¬ìš©í•˜ì„¸ìš”.",
        func=extract_search_keywords
    ),
    Tool(
        name="kipris_patent_searcher", 
        description="KIPRIS íŠ¹í—ˆ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. keyword_extractorë¡œë¶€í„° ë°›ì€ JSON ê²°ê³¼ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.",
        func=search_kipris_patents
    ),
    Tool(
        name="internal_knowledge_searcher",
        description="íšŒì‚¬ ë‚´ë¶€ì˜ V-Belt ê¸°ìˆ  ë¬¸ì„œì™€ ì œí’ˆ ì‚¬ì–‘ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì œí’ˆ ì¶”ì²œì´ë‚˜ ê¸°ìˆ  ì‚¬ì–‘ ë¬¸ì˜ì— ì‚¬ìš©í•˜ì„¸ìš”.",
        func=search_internal_knowledge
    )
]

# LLM ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (LangChain ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ)
if LANGCHAIN_AVAILABLE:
    # LLM ì´ˆê¸°í™” - Qwen3 ëª¨ë¸ ì‚¬ìš© (í•œêµ­ì–´ + ë„êµ¬ í˜¸ì¶œ ìµœì í™”)
    llm = ChatOllama(
        model="qwen3:8b",  # í•œêµ­ì–´ + ë„êµ¬ í˜¸ì¶œì„ ì˜ ì§€ì›í•˜ëŠ” ìµœì‹  ëª¨ë¸
        temperature=0.1,
        num_predict=2048
    )

# ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ (ë„êµ¬ í˜¸ì¶œ ë¯¸ì§€ì› ëª¨ë¸ìš©)
class SimpleAgent:
    """ë„êµ¬ í˜¸ì¶œì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì„ ìœ„í•œ ê°„ë‹¨í•œ ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm, tools):
        self.llm = llm
        self.tools = {tool.name: tool for tool in tools}
        self.tool_descriptions = "\n".join([f"- {tool.name}: {tool.description}" for tool in tools])
    
    def parse_and_execute_action(self, response_text: str) -> tuple[str, str]:
        """ì‘ë‹µì—ì„œ ë„êµ¬ í˜¸ì¶œì„ íŒŒì‹±í•˜ê³  ì‹¤í–‰"""
        
        # ë„êµ¬ í˜¸ì¶œ íŒ¨í„´ ë§¤ì¹­
        import re
        
        # "ë„êµ¬ëª…(ì¸ì)" í˜•íƒœ ë§¤ì¹­
        tool_pattern = r'(\w+)\s*\(\s*["\']([^"\']*)["\']?\s*\)'
        match = re.search(tool_pattern, response_text)
        
        if match:
            tool_name = match.group(1)
            tool_input = match.group(2)
            
            if tool_name in self.tools:
                try:
                    result = self.tools[tool_name].func(tool_input)
                    return tool_name, result
                except Exception as e:
                    return tool_name, f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
        
        # íŠ¹í—ˆ ê²€ìƒ‰ ì˜ë„ ê°ì§€
        if any(keyword in response_text for keyword in ['íŠ¹í—ˆ', 'ê²€ìƒ‰', 'KIPRIS']):
            if 'keyword_extractor' not in response_text:
                # í‚¤ì›Œë“œ ì¶”ì¶œ ë¨¼ì € ì‹¤í–‰
                keyword_result = self.tools['keyword_extractor'].func(response_text)
                patent_result = self.tools['kipris_patent_searcher'].func(keyword_result)
                return 'kipris_patent_searcher', patent_result
        
        # ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ì˜ë„ ê°ì§€  
        elif any(keyword in response_text for keyword in ['ë²¨íŠ¸', 'ëª¨ë¸', 'ì‚¬ì–‘', 'ì œí’ˆ']):
            result = self.tools['internal_knowledge_searcher'].func(response_text)
            return 'internal_knowledge_searcher', result
        
        return None, None
    
    def invoke(self, query_data: dict) -> dict:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰"""
        
        user_input = query_data.get("input", "")
        
        # ì§ì ‘ì ì¸ ë„êµ¬ ì‹¤í–‰ ë°©ì‹
        try:
            # íŠ¹í—ˆ ê²€ìƒ‰ ìš”ì²­ ê°ì§€
            if any(keyword in user_input.lower() for keyword in ['íŠ¹í—ˆ', 'ê²€ìƒ‰', 'kipris']):
                logger.info("íŠ¹í—ˆ ê²€ìƒ‰ ìš”ì²­ ê°ì§€")
                
                # 1ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
                keyword_result = self.tools['keyword_extractor'].func(user_input)
                logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼: {keyword_result}")
                
                # 2ë‹¨ê³„: íŠ¹í—ˆ ê²€ìƒ‰
                patent_result = self.tools['kipris_patent_searcher'].func(keyword_result)
                
                return {"output": patent_result}
            
            # ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ìš”ì²­ ê°ì§€
            elif any(keyword in user_input.lower() for keyword in ['ë²¨íŠ¸', 'ëª¨ë¸', 'ì‚¬ì–‘', 'ì œí’ˆ', 'ì¶”ì²œ']):
                logger.info("ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ìš”ì²­ ê°ì§€")
                
                result = self.tools['internal_knowledge_searcher'].func(user_input)
                return {"output": result}
            
            else:
                # ì¼ë°˜ ì‘ë‹µ
                return {"output": f"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì „ë™ë²¨íŠ¸ ë° ì»¨ë² ì´ì–´ë²¨íŠ¸ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\në‹¤ìŒê³¼ ê°™ì€ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\nâ€¢ KIPRIS íŠ¹í—ˆ ê²€ìƒ‰ (ì˜ˆ: 'íƒ€ì´ë°ë²¨íŠ¸ íŠ¹í—ˆ ì°¾ì•„ì¤˜')\nâ€¢ ë‚´ë¶€ ì œí’ˆ ì‚¬ì–‘ ë° ê¸°ìˆ  ë¬¸ì„œ ê²€ìƒ‰ (ì˜ˆ: 'ê³ ì˜¨ìš© ë²¨íŠ¸ ì¶”ì²œí•´ì¤˜')\n\nêµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."}
                
        except Exception as e:
            logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return {"output": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

# LLM ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (LangChain ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ)
if LANGCHAIN_AVAILABLE:
    # LLM ì´ˆê¸°í™” - ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ í•œêµ­ì–´ ëª¨ë¸ë¡œ ì„¤ì •
    llm = ChatOllama(
        model="my-korean-llm:latest",  # ì‚¬ìš©ìì˜ í•œêµ­ì–´ ëª¨ë¸
        temperature=0.1,
        num_predict=2048
    )

    # ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸ ìƒì„± (ë„êµ¬ í˜¸ì¶œ ê¸°ëŠ¥ ì—†ì´)
    try:
        agent_executor = SimpleAgent(llm, tools)
        logger.info("ì»¤ìŠ¤í…€ SimpleAgentê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.warning(f"ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}. ê¸°ë³¸ ì‘ë‹µ ëª¨ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        agent_executor = None
else:
    agent_executor = None
    print("Warning: LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì‘ë‹µ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

@app.post("/ask")
async def ask_question(request: Request):
    try:
        data = await request.json()
        user_input = data.get("input", "")
        chat_history = data.get("chat_history", [])
        
        if not user_input.strip():
            return {"response": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}
        
        logger.info(f"ì‚¬ìš©ì ì§ˆë¬¸: {user_input}")
        
        # LangChain ì—ì´ì „íŠ¸ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
        if agent_executor and LANGCHAIN_AVAILABLE:
            result = agent_executor.invoke({
                "input": user_input,
                "chat_history": chat_history
            })
            response = result.get("output", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ê¸°ë³¸ ì‘ë‹µ ëª¨ë“œ (LangChain ì—†ì´)
            response = handle_basic_query(user_input)
        
        logger.info(f"ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        return {"response": response}
        
    except Exception as e:
        logger.error(f"ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"response": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

def handle_basic_query(user_input: str) -> str:
    """LangChain ì—†ì´ ê¸°ë³¸ì ì¸ ì§ˆë¬¸ ì²˜ë¦¬"""
    
    try:
        # íŠ¹í—ˆ ê²€ìƒ‰ ì˜ë„ ê°ì§€
        if any(keyword in user_input for keyword in ['íŠ¹í—ˆ', 'ê²€ìƒ‰', 'KIPRIS', 'kipris']):
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keyword_result = extract_search_keywords(user_input)
            # íŠ¹í—ˆ ê²€ìƒ‰ ì‹¤í–‰
            search_result = search_kipris_patents(keyword_result)
            return search_result
        
        # ë‚´ë¶€ ì§€ì‹ ê²€ìƒ‰ ì˜ë„ ê°ì§€
        elif any(keyword in user_input for keyword in ['ë²¨íŠ¸', 'ëª¨ë¸', 'ì‚¬ì–‘', 'ì œí’ˆ', 'ì¶”ì²œ']):
            search_result = search_internal_knowledge(user_input)
            return search_result
        
        else:
            return f"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì „ë™ë²¨íŠ¸ ë° ì»¨ë² ì´ì–´ë²¨íŠ¸ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\në‹¤ìŒê³¼ ê°™ì€ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\nâ€¢ KIPRIS íŠ¹í—ˆ ê²€ìƒ‰\nâ€¢ ë‚´ë¶€ ì œí’ˆ ì‚¬ì–‘ ë° ê¸°ìˆ  ë¬¸ì„œ ê²€ìƒ‰\n\nêµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
    
    except Exception as e:
        logger.error(f"ê¸°ë³¸ ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "="*60)
    print("ğŸš€ ì „ë™ë²¨íŠ¸/ì»¨ë² ì´ì–´ë²¨íŠ¸ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì‹œì‘")
    print("="*60)
    
    # ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬
    if not LANGCHAIN_AVAILABLE:
        print("âš ï¸  LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ê³ ê¸‰ ì—ì´ì „íŠ¸ ê¸°ëŠ¥ì„ ìœ„í•´ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install langchain langchain-ollama langchain-core")
        print("í˜„ì¬ëŠ” ê¸°ë³¸ ì‘ë‹µ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    else:
        print("âœ… Qwen3 ëª¨ë¸ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤ (í•œêµ­ì–´ + ë„êµ¬ í˜¸ì¶œ ìµœì í™”).")
    
    if not KONLPY_AVAILABLE:
        print("âš ï¸  KoNLPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ë” ì •í™•í•œ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•´ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install konlpy")
        print("í˜„ì¬ëŠ” ê·œì¹™ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        print("âœ… KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    if not VECTORSTORE_AVAILABLE:
        print("âš ï¸  ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ê³ ê¸‰ RAG ê¸°ëŠ¥ì„ ìœ„í•´ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install langchain-huggingface sentence-transformers faiss-cpu")
        print("í˜„ì¬ëŠ” í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        print("âœ… ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ RAG ì‹œìŠ¤í…œì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("="*60 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)